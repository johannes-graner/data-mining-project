{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class frocc:\n",
    "  def __init__(self, m, eps, d, seed=None) -> None:\n",
    "    if (seed != None):\n",
    "      np.random.seed(seed)\n",
    "    self.m = m\n",
    "    self.eps = eps\n",
    "    self.d = d\n",
    "\n",
    "    # Generate random vectors from d-dim unit sphere\n",
    "    # by x/|x|, x ~ N(0,I_d)\n",
    "    random_vectors = st.multivariate_normal.rvs(np.zeros(d), np.eye(d), m)\n",
    "    self.random_proj = np.divide(\n",
    "      random_vectors, \n",
    "      np.linalg.norm(random_vectors, axis=1).repeat(d).reshape(m,d)\n",
    "    )\n",
    "    self.outlier_intervals = np.array([])\n",
    "    self.min_values = np.array([])\n",
    "    self.max_values = np.array([])\n",
    "    \n",
    "  def train(self, data):\n",
    "    N, d_data = data.shape\n",
    "    if d_data != self.d:\n",
    "      raise ValueError(\"Dimensions must match. d = \" + str(d) + \" != \" + str(d_data) + \" = d_data.\")\n",
    "\n",
    "    # projection onto unit vector = dot product\n",
    "    dot_products = self.random_proj @ data.T\n",
    "    min_values = np.min(dot_products, axis=1)\n",
    "    max_values = np.max(dot_products, axis=1)\n",
    "\n",
    "    # Need to min-max normalize and sort projections to\n",
    "    # create outlier intervals along each projection vector\n",
    "    scaled_dot_products = np.divide(\n",
    "      dot_products - min_values.repeat(N).reshape(self.m,N),\n",
    "      (max_values - min_values).repeat(N).reshape(self.m,N)\n",
    "    )\n",
    "    sorted_scaled_dot_products = np.sort(scaled_dot_products, axis=1)\n",
    "\n",
    "    # Find indices where outlier intervals start\n",
    "    two_d_interval_indices = np.argwhere(\n",
    "      np.diff(sorted_scaled_dot_products, axis=1) >= eps\n",
    "    )\n",
    "    outlier_break_points = [[ \n",
    "      index[1] for index in two_d_interval_indices if index[0] == i \n",
    "    ] for i in range(self.m) ]\n",
    "\n",
    "    self.min_values = min_values\n",
    "    self.max_values = max_values\n",
    "    self.outlier_intervals = [[ \n",
    "      [\n",
    "        sorted_scaled_dot_products[i,index],\n",
    "        sorted_scaled_dot_products[i,index+1]\n",
    "      ] for index in outlier_break_points[i] \n",
    "    ] for i in range(self.m)]\n",
    "\n",
    "  def test(self, data):\n",
    "    N, d_data = data.shape\n",
    "    if d_data != self.d:\n",
    "      raise ValueError(\"Dimensions must match. d = \" + str(d) + \" != \" + str(d_data) + \" = d_data.\")\n",
    "\n",
    "    # Find projection of testing data and rescale\n",
    "    # to the same scale as training data\n",
    "    projection_new_data = self.random_proj @ data.T\n",
    "    scaled_new_data = np.divide(\n",
    "      projection_new_data.reshape(self.m,N) -\n",
    "        self.min_values.repeat(N).reshape(self.m,N),\n",
    "      (self.max_values - self.min_values).repeat(N).reshape(self.m,N)\n",
    "    )\n",
    "\n",
    "    # Outliers are either more extreme than any training data\n",
    "    # or are within some outlier intervals\n",
    "    extreme_upper = scaled_new_data > 1\n",
    "    extreme_lower = scaled_new_data < 0\n",
    "    inliers = np.logical_not(np.any(np.logical_or(extreme_upper, extreme_lower), axis=0))\n",
    "    # print(scaled_new_data.shape)\n",
    "    # print(inliers)\n",
    "\n",
    "    # Check intervals where there are no extreme values\n",
    "    for i in range(self.m):\n",
    "      # Find indices of non-extreme values\n",
    "      non_extreme_indices = np.argwhere(inliers).flatten()\n",
    "      to_check = inliers[non_extreme_indices]\n",
    "\n",
    "      # Retrieve data that must be checked\n",
    "      data_to_check = scaled_new_data[i, non_extreme_indices]\n",
    "      for outlier in self.outlier_intervals[i]:\n",
    "        # For each outlier interval, \n",
    "        # update indices with new outliers\n",
    "        to_check = np.logical_and(\n",
    "          to_check,\n",
    "          np.logical_not(\n",
    "            np.logical_and(\n",
    "              data_to_check >= outlier[0], \n",
    "              data_to_check <= outlier[1]\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "      # Update the global record of inliers.\n",
    "      # When a data point is found to be anomalous,\n",
    "      # it is no longer checked every iteration.\n",
    "      inliers[non_extreme_indices] = to_check\n",
    "\n",
    "    return np.logical_not(inliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = number of random projection vectors\n",
    "# d = data dimension\n",
    "# N = number of data points\n",
    "m = 10\n",
    "d = 1000\n",
    "N_data = 1000\n",
    "N_new = 100\n",
    "eps = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example():\n",
    "  # Generate some training data\n",
    "  np.random.seed(1000)\n",
    "  data = st.multivariate_normal.rvs(np.zeros(d), np.eye(d), size = N_data)\n",
    "\n",
    "  # And some testing data\n",
    "  new_data = st.multivariate_normal.rvs(np.ones(d) * 3, np.eye(d), size = N_new)\n",
    "\n",
    "  detector = frocc(m, eps, d, seed=64)\n",
    "  detector.train(data)\n",
    "  print(detector.test(new_data).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k = 100\\naccuracies = np.zeros(k)\\nfor i in range(k):\\n  detector = frocc(m, eps, d, seed=None)\\n  detector.train(data)\\n  accuracies[i] = detector.test(new_data).mean()\\n\\nplt.hist(accuracies);'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_histogram():\n",
    "  k = 100\n",
    "  accuracies = np.zeros(k)\n",
    "  np.random.seed(1000)\n",
    "  data = st.multivariate_normal.rvs(np.zeros(d), np.eye(d), size = N_data)\n",
    "\n",
    "  # And some testing data\n",
    "  new_data = st.multivariate_normal.rvs(np.ones(d) * 3, np.eye(d), size = N_new)\n",
    "  for i in range(k):\n",
    "    detector = frocc(m, eps, d, seed=None)\n",
    "    detector.train(data)\n",
    "    accuracies[i] = detector.test(new_data).mean()\n",
    "\n",
    "  plt.hist(accuracies);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
